{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "import random\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "import sys\n",
    "import pickle\n",
    "from spellchecker import SpellChecker\n",
    "import re\n",
    "import wordninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = SpellChecker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for processing and calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_loader(file, listify=True):\n",
    "    wrapper = open(file,\"r\",encoding=\"utf-8-sig\")\n",
    "    data = wrapper.readlines()\n",
    "    wrapper.close()\n",
    "    if listify:\n",
    "        data = [line.translate(str.maketrans('', '', punctuation)).strip().split(' ') for line in data]\n",
    "    else:\n",
    "         data = [line.translate(str.maketrans('', '', punctuation)).strip() for line in data]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickleLoad(filename):\n",
    "    start=time.time()\n",
    "    with open(filename, 'rb') as handle:\n",
    "        data = pickle.load(handle)\n",
    "    end=time.time()\n",
    "    print(str(end-start)+': time elapsed(s)')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return True if n_gram contains less than n non-stop words, once false, no more items are added\n",
    "def lenChecker(n_gram,n):\n",
    "    clean = [tok for tok in n_gram if tok not in stop_words]\n",
    "    return len(clean)<n\n",
    "# given an n-gram as a string, returns n\n",
    "def whatGram(n_gram):\n",
    "    clean = n_gram.split(' ')\n",
    "    clean = [tok for tok in clean if tok not in stop_words]\n",
    "    return len(clean)\n",
    "def lenChecker2(n_gram,n):\n",
    "    clean = [tok for tok in n_gram if tok not in stop_words]\n",
    "    return len(clean)==n\n",
    "def lenChecker3(n_gram,n):\n",
    "    clean = [tok for tok in n_gram.split(' ') if tok not in stop_words]\n",
    "    return len(clean)==n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unigrams(collection):\n",
    "    start = time.time()\n",
    "    unigrams = {}\n",
    "    for title in collection:\n",
    "        for word in title:\n",
    "            \n",
    "            if word in stop_words or len(word)==0:\n",
    "                continue\n",
    "            \n",
    "            if word in unigrams:\n",
    "                unigrams[word][0]+=1   \n",
    "            else:\n",
    "                # freq,n\n",
    "                unigrams[word]=[1,1]\n",
    "    end = time.time()\n",
    "    print(str(end-start)+': time elapsed(s)')\n",
    "    return unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bigrams(collection):\n",
    "    start=time.time()\n",
    "    bigrams = {}\n",
    "    for title in collection:\n",
    "        # -1 as last token must be in prior bigram only\n",
    "        for i in range(len(title)-1):\n",
    "            temp=[]\n",
    "            if title[i] in stop_words:\n",
    "                continue\n",
    "            else:\n",
    "                temp.append(title[i])\n",
    "                j=1\n",
    "            while i+j < len(title) and lenChecker(temp,2):\n",
    "                succ = title[i+j]\n",
    "                temp.append(succ)\n",
    "                j+=1\n",
    "\n",
    "            if len(temp)>1:\n",
    "                last = temp[len(temp)-1]\n",
    "                if last in stop_words:\n",
    "                    continue\n",
    "                else:\n",
    "                    temp = ' '.join(temp)\n",
    "                    if temp in bigrams:\n",
    "                        bigrams[temp][0]+=1\n",
    "                    else:\n",
    "                        bigrams[temp]=[1,2]\n",
    "    end = time.time()\n",
    "    print(str(end-start)+': time elapsed(s)')\n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trigrams(collection):\n",
    "    start=time.time()\n",
    "    trigrams = {}\n",
    "    for title in collection:\n",
    "        # -2 as last 2 tokens must be in prior trigram only\n",
    "        for i in range(len(title)-2):\n",
    "            temp=[]\n",
    "            if title[i] in stop_words:\n",
    "                continue\n",
    "            else:\n",
    "                temp.append(title[i])\n",
    "                j=1\n",
    "            while i+j < len(title) and lenChecker(temp,3):\n",
    "                succ = title[i+j]\n",
    "                temp.append(succ)\n",
    "                j+=1\n",
    "\n",
    "            if len(temp)>1:\n",
    "                last = temp[len(temp)-1]\n",
    "                if last in stop_words:\n",
    "                    continue\n",
    "                elif not lenChecker2(temp,3):\n",
    "                    continue;\n",
    "                else:\n",
    "                    temp = ' '.join(temp)\n",
    "                    if temp in trigrams:\n",
    "                        trigrams[temp][0]+=1\n",
    "                    else:\n",
    "                        trigrams[temp]=[1,3]\n",
    "    end = time.time()\n",
    "    print(str(end-start)+': time elapsed(s)')\n",
    "    return trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes stop-words from complete part of query, investigate differences further\n",
    "def query_splitter(query):\n",
    "    query = [a for a in query.split(' ') if a]\n",
    "    # lower tokens\n",
    "   # query = [a.lower() if a not in lc_blacklist else a for a in query]\n",
    "    completed = [a for a in query[:-1] if a not in stop_words]\n",
    "    partial = query[-1:][0]\n",
    "    return [completed, partial]\n",
    "\n",
    "\n",
    "def complete(part):\n",
    "    #start=time.time()\n",
    "    completions=[]\n",
    "    for c in list(unigrams.keys()):\n",
    "        if c.startswith(part):\n",
    "            completions.append(c)\n",
    "    #end = time.time()\n",
    "   # print(str(end-start)+': time elapsed(s)')\n",
    "    return completions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the total number of documents containing the term\n",
    "def df(term):\n",
    "    # only check unigrams\n",
    "    if phrases[term][1]==1:\n",
    "        return phrases[term][0]\n",
    "    else:\n",
    "        return 0\n",
    "# returns the inverse domain frequency of a term\n",
    "# TODO: change small=>titles\n",
    "\n",
    "# experiment with omitting +1, assuming df>0\n",
    "def idf(term):\n",
    "    return np.log(1+len(small)/1+df(term))\n",
    "# needs to be changed not to use small\n",
    "\n",
    "def term_completion_prob(term,denom):\n",
    "    return (df(term)*idf(term))/denom\n",
    "\n",
    "# n-gram as string\n",
    "def freq_norm(n_gram):\n",
    "    return phrases[n_gram][0] / (np.log(averageFreqs[phrases[n_gram][1]]))\n",
    "\n",
    "# n-gram as string\n",
    "def term2phrase(n_gram, term):\n",
    "    #start=time.time()\n",
    "    if term in n_gram:\n",
    "        #out = freq_norm(n_gram) / sum([freq_norm(a) for a in t2p[term][0]])\n",
    "        out = freq_norm(n_gram) / t2p[term][1]\n",
    "    else:\n",
    "        out = 0\n",
    "    #end=time.time()\n",
    "   # print(end-start)\n",
    "    return out\n",
    "\n",
    "# n-gram as string\n",
    "def phrase_selection_prob(n_gram,term,completions,denom):\n",
    "    #return term2phrase(n_gram,term) * term_completion_prob(term,completions,denom)\n",
    "    return term2phrase(n_gram,term) * term_completion_prob(term,completions,denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build Inverted Index for finding the phrases containing a searched term\n",
    "# {term:[phrase]} containing term\n",
    "\n",
    "def t2p_build():\n",
    "    P = {}\n",
    "    start = time.time()\n",
    "    for p in list(phrases.keys()):\n",
    "        for word in p.split(' '):\n",
    "            if word in P:\n",
    "                # p.split(' ')\n",
    "                P[word].append(p)\n",
    "            else:\n",
    "                # p.split(' ')\n",
    "                P[word] = [p]\n",
    "    \n",
    "    # store sum of freq norm for all phrases relating to a term\n",
    "    # so this may be accessed by term2phrase and not calculated at runtime\n",
    "    for k in P:\n",
    "        P[k] = (P[k],sum([freq_norm(a) for a in P[k]]))           \n",
    "    end = time.time()\n",
    "    print(str(end-start)+': time elapsed(s)')\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maps terms to docIDs\n",
    "def t2ID_build():    \n",
    "    start=time.time()\n",
    "    idx = {}\n",
    "    for c1, d in enumerate(small):\n",
    "        #tokens = preprocessor(d[0])\n",
    "        for tok in d:\n",
    "            if tok in stop_words:\n",
    "                continue\n",
    "            if tok in idx:\n",
    "                # inverted index is dict mapping term to list of docIDs\n",
    "                idx[tok].add(c1)\n",
    "            else:\n",
    "                idx[tok] = {c1}\n",
    "    end = time.time()\n",
    "    print(str(end-start)+': time elapsed(s)')\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the docIDs of any documents containing all terms present in an input sequence\n",
    "# unions the set of docIDs for each term to produce a set containing each term in the phrase\n",
    "def getIDs(seq):\n",
    "    phrase = [tok for tok in seq if tok not in stop_words]\n",
    "    if phrase:\n",
    "        try:\n",
    "            out = t2ID[phrase[0]]\n",
    "        except KeyError:\n",
    "            return set()\n",
    "        phrase=phrase[1:]\n",
    "        if phrase:\n",
    "            for i in phrase:\n",
    "                 out = out.intersection(t2ID[i])\n",
    "    else:\n",
    "        return set()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n-gram as str\n",
    "def query_correlation(n_gram, full):\n",
    "    #start=time.time()\n",
    "    phrase_IDs = getIDs(n_gram.split(' '))\n",
    "    full_IDs = getIDs(full)\n",
    "    if full_IDs:\n",
    "        check = all([True if word in n_gram else False for word in full])\n",
    "    if phrase_IDs and check:\n",
    "        #print(len(phrase_IDs.intersection(full_IDs)))\n",
    "        #print(len(phrase_IDs))     \n",
    "        out = len(phrase_IDs.intersection(full_IDs))/len(phrase_IDs)\n",
    "    else:\n",
    "        out = 0\n",
    "    #end=time.time()\n",
    "    #print(str(end-start)+': time elapsed(s)')\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(query):\n",
    "    start=time.time()\n",
    "    full, partial = query_splitter(query)\n",
    "    completions = complete(partial)\n",
    "    # calc ahead of time as denom is constant given query\n",
    "    denom = sum([df(c)*idf(c) for c in completions])\n",
    "    # C will hold candidate phrases and their scores\n",
    "    C=[]\n",
    "    for c in completions:\n",
    "        # iterate over possible word completions\n",
    "        tc_prob = term_completion_prob(c,denom)\n",
    "        for p in t2p[c][0]: \n",
    "            # for each completion, iterate over all phrases containing the completion\n",
    "            t2p_prob = term2phrase(p,c)\n",
    "            if full:\n",
    "                qp_corr = query_correlation(p,full)\n",
    "                score = tc_prob * t2p_prob * qp_corr\n",
    "            else:\n",
    "                 score = tc_prob * t2p_prob\n",
    "            # add phrase and its score to output\n",
    "            if score>0:\n",
    "                C.append((p,score))\n",
    "    # sort by score\n",
    "    C.sort(key=lambda tup: tup[1],reverse=True)\n",
    "    end=time.time()\n",
    "    print(str(end-start)+': time elapsed(s)')\n",
    "    return [a[0] for a in C[:8]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(txt_loader(\"englishST.txt\",False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23845911026000977: time elapsed(s)\n",
      "Example title format: \n",
      "['Easy', 'way', 'to', 'add', 'standardUserDefaults', 'to', 'Crittercism', 'error', 'report', 'metadata']\n"
     ]
    }
   ],
   "source": [
    "# small is our sub-dataset consisting of 100k titles\n",
    "small = pickleLoad('100k.pickle')\n",
    "# remove empty items, occasional titles contain ['']\n",
    "small = [list(filter(None, str_list)) for str_list in small]\n",
    "print('Example title format: \\n'+str(small[2121]))\n",
    "k=50000\n",
    "small = random.sample(small,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9920730590820312: time elapsed(s)\n"
     ]
    }
   ],
   "source": [
    "small = pickleLoad('1m.pickle')\n",
    "k = 80000\n",
    "small = random.sample(small,k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### - SpellCheck terms, identify unknown tokens\n",
    "###### - build Camel case blacklist\n",
    "###### - Camel case handling to split CC tokens\n",
    "###### - Multi-word case handling to split MW tokens : getOrderValue -> get, Order, Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# camelCase blacklist\n",
    "cc_pattern = re.compile(\"[A-Z]*[^A-Z]+\")\n",
    "cc_blacklist = ['jQuery','MySQL','JavaScript']\n",
    "lc_blacklist = ['I',\"I'm\",\"I'll\",\"I'd\"]\n",
    "sc = SpellChecker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split camel case tokens\n",
    "for title in small:\n",
    "    knowns = sc.known(title)\n",
    "    # get unknown tokens and their indexes\n",
    "    unknowns = [tok for tok in title if tok.lower() not in knowns]\n",
    "    for i in unknowns:\n",
    "        # check if token is camelCase and can be split\n",
    "        if cc_pattern.findall(i) and i not in cc_blacklist:\n",
    "            idx = title.index(i)\n",
    "            title.remove(i)\n",
    "            for c2,j in enumerate(cc_pattern.findall(i)):\n",
    "                title.insert(idx+c2,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split multi word tokens\n",
    "for title in small:\n",
    "    knowns = sc.known(title)\n",
    "    # get unknown tokens and their indexes\n",
    "    unknowns = [tok for tok in title if tok.lower() not in knowns]\n",
    "    for i in unknowns:\n",
    "        if wordninja.split(i):\n",
    "            idx = title.index(i)\n",
    "            title.remove(i)\n",
    "            for c,tok in enumerate(wordninja.split(i)):\n",
    "                title.insert(idx+c,tok)\n",
    "\n",
    "small = [[a.lower() if a not in lc_blacklist else a for a in title] for title in small]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build models/structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we build our n-grams, calc the average frequency for each n, and add into averageFreqs\n",
    "averageFreqs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = build_unigrams(small)\n",
    "averageFreqs[1] = sum([a[0] for a in list(unigrams.values())])/(len(list(unigrams.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = build_bigrams(small)\n",
    "averageFreqs[2] = sum([a[0] for a in list(bigrams.values())])/(len(list(bigrams.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams=build_trigrams(small)\n",
    "averageFreqs[3] = sum([a[0] for a in list(trigrams.values())])/(len(list(trigrams.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('unigrams/vocab size: '+str(len(unigrams.keys())))\n",
    "print('bigrams: '+str(len(bigrams.keys())) )\n",
    "print( 'trigrams: '+str(len(trigrams.keys())) +'\\n')\n",
    "print('Average freq of n-grams for each order n:\\n'+str(averageFreqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phrases maps {n-gram:[freq,n]}\n",
    "phrases = {}\n",
    "phrases.update(unigrams)\n",
    "phrases.update(bigrams)\n",
    "phrases.update(trigrams)\n",
    "\n",
    "# bigrams and trigrams no longer needed, can be freed from memory, retain unigrams to serve as vocab\n",
    "del bigrams, trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.802832126617432: time elapsed(s)\n"
     ]
    }
   ],
   "source": [
    "# build term to [phrase] index\n",
    "t2p = t2p_build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3256101608276367: time elapsed(s)\n"
     ]
    }
   ],
   "source": [
    "# build term to [docID] index\n",
    "t2ID = t2ID_build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run / Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5543758869171143: time elapsed(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['postgres sql language',\n",
       " 'variable value to postgres ltree',\n",
       " 'postgres ltree column',\n",
       " 'log using triggers in postgres',\n",
       " 'TCPNODELAY for libpq and postgres',\n",
       " 'libpq and postgres server',\n",
       " 'large file in postgres',\n",
       " 'postgres ltree']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(\"postgres l\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.277764081954956: time elapsed(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['secure than JavaScript solutions',\n",
       " 'secure HTTP server',\n",
       " 'secure socket server',\n",
       " 'reactJs secure storage',\n",
       " 'secure my data in sdcard',\n",
       " 'method for secure socket',\n",
       " 'secure socket server',\n",
       " 'Pass session through secure']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(\"secure s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6444478034973145: time elapsed(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['sql server management',\n",
       " 'microsoft sql server',\n",
       " 'ms sql server',\n",
       " 'ms sql',\n",
       " 'sql server mode',\n",
       " 'ms sql query',\n",
       " 'ms sql database',\n",
       " 'php ms sql']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(\"sql m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from original paper - \"To summarize our approach, given a user query, the system first finds all the possible completions of the last query word. These completions are then used to identify a set of phrases that can be used for generating possible query suggestions. All the phrases in this candidate set are then assigned a probability score using equation 7. The top 10 highest scoring phrases are then presented to the user after appending to the Qc portion of the user query.\" - \"http://sumitbhatia.net/papers/sigir11.pdf\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
